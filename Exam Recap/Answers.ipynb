{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca8eb04-a190-4f5c-b883-f31d03532798",
   "metadata": {},
   "source": [
    "# Exam Preparation Q & A\n",
    "\n",
    "---\n",
    "\n",
    "## Multiple Linear Regression\n",
    "\n",
    "**Q1.** What is the main assumption behind multiple linear regression?\n",
    "\n",
    "**A.** The expected response is a *linear* combination of the predictors (after any chosen transforms), and the errors are i.i.d., homoscedastic, normal, and uncorrelated, with no perfect multicollinearity.\n",
    "\n",
    "\n",
    "**Q2.** How is the loss function in multiple linear regression defined?\n",
    "\n",
    "**A.** Ordinary‑Least‑Squares minimises the mean‑squared error\n",
    "$\\mathcal{L}(\\boldsymbol\\beta)=\\frac{1}{n}\\sum_{i=1}^{n}\\bigl(y_i-\\mathbf x_i^{\\!\\top}\\boldsymbol\\beta\\bigr)^2.$\n",
    "\n",
    "\n",
    "**Q3.** When should multiple linear regression be avoided?\n",
    "\n",
    "**A.** When the relationship is highly non‑linear, residual variance is non‑constant, predictors are strongly collinear, influential outliers dominate, or $p\\gg n$.\n",
    "\n",
    "\n",
    "**Q4.** How do you interpret the coefficients of a linear regression model?\n",
    "\n",
    "**A.** Each $\\beta_j$ is the expected change in $y$ for a one‑unit increase in $x_j$ **holding all other predictors constant**.\n",
    "\n",
    "---\n",
    "\n",
    "## Polynomial & Spline Regression\n",
    "\n",
    "**Q5.** Why use polynomial regression over linear regression?\n",
    "\n",
    "**A.** It captures smooth curvature while keeping a model that is linear in the parameters.\n",
    "\n",
    "\n",
    "**Q6.** What is the danger of using high‑degree polynomials?\n",
    "\n",
    "**A.** High variance: wild oscillations between data points, poor extrapolation, and multicollinearity.\n",
    "\n",
    "\n",
    "**Q7.** How do splines improve over high‑degree polynomials?\n",
    "\n",
    "**A.** They fit *piece‑wise* low‑degree polynomials joined smoothly at knots, so flexibility is local and stable.\n",
    "\n",
    "---\n",
    "\n",
    "## Ridge & Lasso Regression\n",
    "\n",
    "**Q8.** What is the purpose of regularisation?\n",
    "\n",
    "**A.** Add a penalty to shrink coefficients, reducing overfitting and multicollinearity.\n",
    "\n",
    "\n",
    "**Q9.** How does Ridge differ from Lasso regression?\n",
    "\n",
    "**A.** Ridge ($L_2$) shrinks all coefficients; Lasso ($L_1$) can set some exactly to zero, giving sparsity.\n",
    "\n",
    "\n",
    "**Q10.** Which regularisation technique can be used for feature selection?\n",
    "\n",
    "**A.** Lasso (or Elastic Net).\n",
    "\n",
    "\n",
    "**Q11.** What does the **alpha** hyper‑parameter control in Ridge and Lasso?\n",
    "\n",
    "**A.** Penalty strength: larger $\\alpha$ ⇒ stronger shrinkage / more zeros; $\\alpha=0$ recovers OLS.\n",
    "\n",
    "---\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "**Q12.** What type of problem does logistic regression solve?\n",
    "\n",
    "**A.** Classification (binary, with soft‑max extension to multi‑class).\n",
    "\n",
    "\n",
    "**Q13.** What is the output of a logistic regression model?\n",
    "\n",
    "**A.** The estimated probability\n",
    "$\\hat p = \\sigma\\bigl(\\mathbf x^{\\!\\top}\\boldsymbol\\beta\\bigr)=\\frac{1}{1+e^{-\\mathbf x^{\\!\\top}\\boldsymbol\\beta}}.$\n",
    "\n",
    "\n",
    "**Q14.** How does F1‑score help with imbalanced datasets?\n",
    "\n",
    "**A.** $F_1 = 2\\,\\frac{PR}{P+R}$ balances precision $P$ and recall $R$, so majority‑class accuracy doesn’t dominate.\n",
    "\n",
    "\n",
    "**Q15.** Scenario where recall > precision is more important and why?\n",
    "\n",
    "**A.** Medical screening: missing a sick patient (low recall) is costlier than extra false positives (lower precision).\n",
    "\n",
    "---\n",
    "\n",
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "**Q17.** What kind of data are CNNs best suited for?\n",
    "\n",
    "**A.** Grid‑like data with local spatial correlations (images, video frames, spectrograms, 1‑D/3‑D signals).\n",
    "\n",
    "\n",
    "**Q18.** Name two common layers used in CNNs.\n",
    "\n",
    "**A.** Convolutional layers and pooling (max or average) layers.\n",
    "\n",
    "\n",
    "**Q19.** Name parameters used for defining a CNN.\n",
    "\n",
    "**A.** Number & size of filters, kernel size, stride, padding, number of conv–pool blocks, learning rate, weight decay, batch size.\n",
    "\n",
    "\n",
    "**Q20.** What is the role of filters in CNNs?\n",
    "\n",
    "**A.** Learn local patterns (edges → textures → objects); stacking filters builds hierarchical features.\n",
    "\n",
    "---\n",
    "\n",
    "## Recurrent Neural Networks (RNNs)\n",
    "\n",
    "**Q21.** What makes RNNs different from feed‑forward networks?\n",
    "\n",
    "**A.** They pass a hidden state from one time‑step to the next, giving memory of prior inputs.\n",
    "\n",
    "\n",
    "**Q22.** What is a limitation of typical RNNs?\n",
    "\n",
    "**A.** Vanishing/exploding gradients, so they struggle with long‑range dependencies (mitigated by LSTM/GRU).\n",
    "\n",
    "\n",
    "**Q23.** What data types are RNNs commonly used for?\n",
    "\n",
    "**A.** Sequential data: text, speech, sensor or financial time‑series, DNA.\n",
    "\n",
    "\n",
    "**Q24.** Give three examples of sequential problems.\n",
    "\n",
    "**A.** Language modelling, machine translation, stock‑price prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## Ensemble Methods\n",
    "\n",
    "**Q25.** What is the core idea behind ensemble methods?\n",
    "\n",
    "**A.** Combine multiple diverse learners so their uncorrelated errors cancel out.\n",
    "\n",
    "\n",
    "**Q26.** What are the main types of ensemble methods?\n",
    "\n",
    "**A.** Bagging (e.g. Random Forest), Boosting (Ada/Gradient/XGBoost), Stacking/Voting.\n",
    "\n",
    "\n",
    "**Q27.** What is stacking in ensemble learning?\n",
    "\n",
    "**A.** Base models are trained; their out‑of‑fold predictions feed a meta‑learner that blends them.\n",
    "\n",
    "\n",
    "**Q28.** What are the advantages of using ensemble models?\n",
    "\n",
    "**A.** Higher accuracy, lower variance, greater robustness, better feature‑importance stability.\n",
    "\n",
    "---\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "**Q29.** How does a Random Forest differ from a decision tree?\n",
    "\n",
    "**A.** It builds many trees on bootstrap samples with random feature splits and averages/votes the results.\n",
    "\n",
    "\n",
    "**Q30.** What is the role of **max\\_features** in Random Forest?\n",
    "\n",
    "**A.** Sets how many predictors each split may test; smaller values decorrelate trees (reduce variance).\n",
    "\n",
    "\n",
    "**Q31.** What is an out‑of‑bag (OOB) sample?\n",
    "\n",
    "**A.** Data not included in a tree’s bootstrap draw (\\~⅓); used for internal error and feature‑importance estimates.\n",
    "\n",
    "\n",
    "**Q32.** What does increasing **n\\_estimators** typically do?\n",
    "\n",
    "**A.** Reduces variance and OOB/test error until a plateau; after that it mostly increases compute cost.\n",
    "\n",
    "---\n",
    "\n",
    "## Boosting\n",
    "\n",
    "**Q33.** How does boosting work conceptually?\n",
    "\n",
    "**A.** Learners are added sequentially, each focusing more on the mistakes of its predecessors.\n",
    "\n",
    "\n",
    "**Q34.** Impact of learners on the final prediction in AdaBoost?\n",
    "\n",
    "**A.** Each weak learner gets weight\n",
    "$\\alpha_m = \\ln\\!\\bigl(\\tfrac{1-\\text{err}_m}{\\text{err}_m}\\bigr);$ better learners thus influence more.\n",
    "\n",
    "\n",
    "**Q35.** What is the role of **learning\\_rate** in boosting?\n",
    "\n",
    "**A.** Scales each learner’s contribution; small values slow learning and act as regularisation.\n",
    "\n",
    "\n",
    "**Q36.** Why is boosting more prone to overfitting than bagging?\n",
    "\n",
    "**A.** It keeps fitting hard/noisy points, especially if base learners are too complex.\n",
    "\n",
    "---\n",
    "\n",
    "## DBSCAN\n",
    "\n",
    "**Q37.** What kind of clustering does DBSCAN perform?\n",
    "\n",
    "**A.** Density‑based clustering.\n",
    "\n",
    "\n",
    "**Q38.** What does the **eps** parameter control in DBSCAN?\n",
    "\n",
    "**A.** Neighbourhood radius within which points are considered neighbours of a *core* point.\n",
    "\n",
    "\n",
    "**Q39.** How does DBSCAN handle outliers?\n",
    "\n",
    "**A.** Points not density‑reachable from any core point are labelled *noise*.\n",
    "\n",
    "\n",
    "**Q40.** What type of datasets is DBSCAN ideal for?\n",
    "\n",
    "**A.** Arbitrary‑shaped clusters, noise, unknown $k$, low/medium‑dimensional numeric data.\n",
    "\n",
    "---\n",
    "\n",
    "## UMAP\n",
    "\n",
    "**Q41.** What is the primary purpose of UMAP?\n",
    "\n",
    "**A.** Non‑linear dimensionality reduction / visualisation that preserves local and some global structure.\n",
    "\n",
    "\n",
    "**Q42.** What does **n\\_neighbors** control in UMAP؟\n",
    "\n",
    "**A.** Trade‑off between local detail (small values) and global structure (large values).\n",
    "\n",
    "\n",
    "**Q43.** Common application of UMAP?\n",
    "\n",
    "**A.** Visualising single‑cell RNA‑seq, image or word embeddings; producing embeddings for clustering.\n",
    "\n",
    "\n",
    "**Q44.** Is UMAP deterministic?\n",
    "\n",
    "**A.** Not strictly; fixing the random seed gives repeatable results.\n",
    "\n",
    "---\n",
    "\n",
    "## Multidimensional Scaling (MDS)\n",
    "\n",
    "**Q45.** What is the main idea of MDS?\n",
    "\n",
    "**A.** Place points in low‑D so that pairwise Euclidean distances approximate a given dissimilarity matrix.\n",
    "\n",
    "\n",
    "**Q46.** Difference between metric and non‑metric MDS?\n",
    "\n",
    "**A.** Metric preserves exact distances; non‑metric preserves only their rank order via a monotone transform.\n",
    "\n",
    "\n",
    "**Q47.** Example when metric MDS is recommended.\n",
    "\n",
    "**A.** When dissimilarities are true Euclidean distances, e.g. great‑circle distances between cities.\n",
    "\n",
    "\n",
    "**Q48.** Computational speed of MDS vs. UMAP/t‑SNE?\n",
    "\n",
    "**A.** Classical MDS needs an $O(n^3)$ eigendecomposition, so it is much slower on large $n$.\n",
    "\n",
    "---\n",
    "\n",
    "## Support‑Vector Machines (SVM)\n",
    "\n",
    "**Q49.** What does the SVM algorithm aim to maximise?\n",
    "\n",
    "**A.** The margin (minimum distance) between classes.\n",
    "\n",
    "\n",
    "**Q50.** What is a support vector?\n",
    "\n",
    "**A.** A training point lying on or inside the margin that defines the decision boundary.\n",
    "\n",
    "\n",
    "**Q51.** What does the kernel trick allow in SVMs?\n",
    "\n",
    "**A.** Computes inner products in high/infinite‑D feature space without explicit mapping, enabling non‑linear separation.\n",
    "\n",
    "\n",
    "**Q52.** What does the hyper‑parameter **C** control?\n",
    "\n",
    "**A.** Soft‑margin penalty: small $C$ ⇒ wider margin, higher bias; large $C$ ⇒ narrow margin, lower bias but risk of overfitting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
